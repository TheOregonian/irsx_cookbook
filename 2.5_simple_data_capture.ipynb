{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import with a generic data structure\n",
    "\n",
    "This is similar to number 2, but starts using a structured data format to capture variables, which helps with explicit defaults.\n",
    "There's not much advantage to doing it this way--arguable the code is more complex--but it starts to become more useful with more complex processing; see recipe 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This format for this data structure is:\n",
    "# data_capture_dict = {\n",
    "#    '<SCHEDULE NAME>': {\n",
    "#        'groups': {\n",
    "#            '<REPEATING GROUP (TABLE) NAME>': {\n",
    "#                '<IRSX VARIABLE NAME': {\n",
    "#                                    'header':'<HEADER IN OUR OUTPUT CSV FILE>',\n",
    "#                                    'default': <DEFAULT VALUE TO USE IF IT'S MISSING\n",
    " \n",
    "\n",
    "data_capture_dict = {\n",
    "    'IRS990ScheduleJ': {\n",
    "        'groups': {\n",
    "            'SkdJRltdOrgOffcrTrstKyEmpl': {\n",
    "                'PrsnNm': {'header':'name'},\n",
    "                'BsnssNmLn1Txt': {'header':'business_name1'},\n",
    "                'BsnssNmLn2Txt': {'header':'business_name2'},\n",
    "                'TtlTxt': {'header':'title'},\n",
    "                'TtlCmpnstnFlngOrgAmt': {\n",
    "                    'header':'org_comp',\n",
    "                    'default':0\n",
    "                },\n",
    "                'TtlCmpnstnRltdOrgsAmt': {\n",
    "                    'header':'related_comp',\n",
    "                    'default':0\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "from irsx.xmlrunner import XMLRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the whole file in here, it's not very long\n",
    "file_rows = [] \n",
    "\n",
    "# We're using the output of part 1\n",
    "with open('pdxefilers.csv', 'rb') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        file_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the output file\n",
    "outfilename =\"employees.csv\"\n",
    "outfile = open(outfilename , 'wb')\n",
    "\n",
    "# the header rows as they'll appear in the output\n",
    "headers = [\"period\", \"ein\", \"object_id\", \"taxpayer_name\", \"name\", \"business_name1\", \"business_name2\", \"title\", \"org_comp\", \"related_comp\"]\n",
    "\n",
    "# start up a dictwriter, ignore extra rows\n",
    "dw = csv.DictWriter(outfile, headers, extrasaction='ignore')\n",
    "dw.writeheader()\n",
    "\n",
    "# get an XMLRunner -- this is what actually does the parsin\n",
    "xml_runner = XMLRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out what to extract\n",
    "\n",
    "Data from each repeating group should go to it's own file, otherwise it won't make sense.\n",
    "\n",
    "To figure out what to capture, I started by looking at schedule J: http://www.irsx.info/#IRS990ScheduleJ\n",
    "Then I went to the table details and picked the rows I wanted from the repeating group:\n",
    "http://www.irsx.info/metadata/groups/SkdJRltdOrgOffcrTrstKyEmpl.html\n",
    "\n",
    "Note that it's common for director/employee names in schedule J to get listed as businessname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_filing(filing, metadata_row, dw):\n",
    "        parsed_filing = xml_runner.run_filing(filing)\n",
    "        if not parsed_filing:\n",
    "            print(\"Skipping filing %s(filings with pre-2013 filings are skipped)\\n row details: %s\" % (filing, metadata_row))\n",
    "            return None\n",
    "        \n",
    "        schedule_list = parsed_filing.list_schedules()\n",
    "\n",
    "        for sked in data_capture_dict.keys():\n",
    "                \n",
    "            if sked in schedule_list:\n",
    "\n",
    "                parsed_skeds = parsed_filing.get_parsed_sked(sked)\n",
    "                if parsed_skeds:\n",
    "                    parsed_sked = parsed_skeds[0]\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                for group in data_capture_dict[sked]['groups']:\n",
    "                    #print(\"Extracting from repeating group %s\" % group)\n",
    "                    try:\n",
    "                        groups = parsed_sked['groups'][group]\n",
    "                        #print(\"Found %s groups for %s\" % (len(groups), group))\n",
    "                    except KeyError:\n",
    "                        print(\"No groups found for %s\\n\" % group)\n",
    "                        continue\n",
    "                    \n",
    "                    # Get the individual variables we're gonna pull \n",
    "                    capture_dict = data_capture_dict[sked]['groups'][group]\n",
    "\n",
    "                    # We know the grops are there, extract from each one\n",
    "                    for parsed_group in groups:\n",
    "                        \n",
    "                        # Store the data for the new csv output file here\n",
    "                        row_data = {}\n",
    "                        # Get rows from the metadata row we passed in\n",
    "                        row_data['period'] = metadata_row['TAX_PERIOD_x']\n",
    "                        row_data['ein'] = metadata_row['EIN']\n",
    "                        row_data['object_id'] = metadata_row['OBJECT_ID']\n",
    "                        row_data['taxpayer_name'] = metadata_row['TAXPAYER_NAME']\n",
    "                        \n",
    "                        for variablename in capture_dict.keys():\n",
    "                            try:\n",
    "                                val = parsed_group[variablename]\n",
    "                                csv_header = capture_dict[variablename]['header']\n",
    "                                row_data[csv_header] = val\n",
    "                            except KeyError:\n",
    "                                \n",
    "                                try:\n",
    "                                    default = capture_dict[variablename]['default']\n",
    "                                    csv_header = capture_dict[variablename]['header']\n",
    "                                    row_data[csv_header]=default\n",
    "                                except KeyError:\n",
    "                                    pass            \n",
    "                        dw.writerow(row_data)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 filings\n",
      "Processed 100 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 200 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 300 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 400 filings\n",
      "Processed 500 filings\n",
      "Processed 600 filings\n",
      "Processed 700 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 800 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 900 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n",
      "Processed 1000 filings\n",
      "No groups found for SkdJRltdOrgOffcrTrstKyEmpl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEMO_MAX = 1000\n",
    "for count, row in enumerate(file_rows):\n",
    "    this_object_id = row['OBJECT_ID']\n",
    "    run_filing(this_object_id, row, dw)\n",
    "    # Don't run endlessly during a demo:\n",
    "    if(count > DEMO_MAX):\n",
    "        break\n",
    "    if count%100==0:\n",
    "        print(\"Processed %s filings\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
