{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump salaries from 990EZ in OR (continuing from part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "from irsx.xmlrunner import XMLRunner\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filers = pd.read_csv(\"orefilers.csv\")\n",
    "# ez = filers[\"RETURN_TYPE\"] == \"990EZ\"\n",
    "# reader = filers[ez]\n",
    "# reader.to_csv('orefilers_ez.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the whole file of orgs with efilings from part 1 here, it's not very long\n",
    "file_rows = [] \n",
    "# We're using the output of part 1\n",
    "with open('orefilers.csv', 'rb') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        return_type = row['RETURN_TYPE']\n",
    "        if return_type == \"990EZ\":\n",
    "            file_rows.append(row)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the output file\n",
    "outfilename =\"ez_comp_1.csv\"\n",
    "outfile = open(outfilename , 'wb')\n",
    "\n",
    "# the header rows as they'll appear in the output\n",
    "headers = [\"ein\", \"object_id\", \"exec_comp\"]\n",
    "# start up a dictwriter, ignore extra rows\n",
    "dw = csv.DictWriter(outfile, headers, extrasaction='ignore')\n",
    "dw.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an XMLRunner -- this is what actually does the parsing\n",
    "xml_runner = XMLRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out what to extract\n",
    "\n",
    "Data from each repeating group should go to it's own file, otherwise it won't make sense.\n",
    "\n",
    "To figure out what to capture, I started by looking at schedule J: http://www.irsx.info/#IRS990ScheduleJ\n",
    "Then I went to the table details and picked the rows I wanted from the repeating group:\n",
    "http://www.irsx.info/metadata/groups/SkdJRltdOrgOffcrTrstKyEmpl.html\n",
    "\n",
    "Note that it's common for director/employee names in schedule J to get listed as businessname.\n",
    "\n",
    "Also note that IRSx checks to see if a file has been downloaded before fetching it. Running this the first time will be slow if the filings aren't already downloaded, but much faster if they've already been downloaded.\n",
    "\n",
    "-------NOTE--------\n",
    "Edited to get compensation from EZ partIV, not sked j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 filings\n",
      "Processed 200 filings\n",
      "Processed 300 filings\n",
      "Processed 400 filings\n",
      "Processed 500 filings\n",
      "Processed 600 filings\n",
      "Processed 700 filings\n",
      "Processed 800 filings\n",
      "Processed 900 filings\n",
      "Processed 1000 filings\n",
      "Processed 1100 filings\n",
      "Processed 1200 filings\n",
      "Processed 1300 filings\n",
      "Processed 1400 filings\n",
      "Processed 1500 filings\n",
      "Processed 1600 filings\n",
      "Processed 1700 filings\n",
      "Processed 1800 filings\n",
      "Processed 1900 filings\n"
     ]
    }
   ],
   "source": [
    "DEMO_MAX = 40\n",
    "num_rows = 0\n",
    "\n",
    "for row in file_rows:\n",
    "    num_rows += 1\n",
    "    this_object_id = row['OBJECT_ID']\n",
    "    parsed_filing = xml_runner.run_filing(this_object_id)\n",
    "    \n",
    "    # if it somehow busted, just note it and continue\n",
    "    if not parsed_filing:\n",
    "        print(\"Skipping filing %s(filings with pre-2013 schemas are skipped)\\n row details: %s\" % (this_object_id, row))\n",
    "        continue \n",
    "    \n",
    "    schedule_list = parsed_filing.list_schedules()\n",
    "    exec_comp = 0\n",
    "    if 'IRS990EZ' in schedule_list:\n",
    "        \n",
    "        \n",
    "        # store the output in this dict\n",
    "        outputdata = {}\n",
    "        # assign some initial values from the input csv\n",
    "        outputdata['ein'] = row['EIN']\n",
    "        outputdata['object_id'] = row['OBJECT_ID']\n",
    "        \n",
    "        # some schedules can appear multiple times, but irs990 only appears once\n",
    "        # so we grab the first one \n",
    "        parsed_main = parsed_filing.get_parsed_sked('IRS990EZ')[0] \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # now use the table name we looked up -- that's where we find it\n",
    "        try:\n",
    "            # repeating groups are returned as an array of dicts\n",
    "            employee_groups = parsed_main['groups']['EZOffcrDrctrTrstEmpl']\n",
    "        except KeyError:\n",
    "            print(\"No EZCmpnstnHghstPdEmpl found in %s skipping\" % this_object_id)\n",
    "            continue\n",
    "          \n",
    "        # read through each employee and pull out the data we want\n",
    "        for employee_group in employee_groups:\n",
    "            \n",
    "            # That leaves the following values to come from schedule J if there is one\n",
    "            # \"name\", \"business_name1\", \"business_name2\", \"title\", \"org_comp\", \"related_comp\"\n",
    "            # those keys come from the headers we gave dictwriter before\n",
    "#             outputdata['name'] = employee_group.get('PrsnNm')\n",
    "#             outputdata['title'] = employee_group.get('TtlTxt')\n",
    "#             outputdata['avg_hours'] = employee_group.get('AvrgHrsPrWkDvtdTPsRt')  \n",
    "#             outputdata['comp_amt'] = employee_group.get('CmpnstnAmt')\n",
    "            comp = int(employee_group.get('CmpnstnAmt'))\n",
    "            if comp>0:\n",
    "                exec_comp += comp\n",
    "            if employee_group.get('EmplyBnftPrgrmAmt'):\n",
    "                exec_comp += int(employee_group.get('EmplyBnftPrgrmAmt'))\n",
    "            if employee_group.get('ExpnsAccntOthrAllwncAmt'):\n",
    "                exec_comp += int(employee_group.get('ExpnsAccntOthrAllwncAmt'))\n",
    "\n",
    "#             ben = int(employee_group.get('EmplyBnftPrgrmAmt'))\n",
    "#             xpns = int(employee_group.get('ExpnsAccntOthrAllwncAmt'))\n",
    "#             if ben:\n",
    "#                 print(this_object_id, ben)\n",
    "#             print(this_object_id +\" \" + employee_group.get('EmplyBnftPrgrmAmt'))\n",
    "#             print(this_object_id +\" \" + employee_group.get('ExpnsAccntOthrAllwncAmt'))\n",
    "#             exec_comp += int(employee_group.get('CmpnstnAmt'))\n",
    "#             exec_comp += int(employee_group.get('EmplyBnftPrgrmAmt'))\n",
    "#             exec_comp += int(employee_group.get('ExpnsAccntOthrAllwncAmt'))\n",
    "        \n",
    "#             print(\"filing %s\" % this_object_id)\n",
    "#     else:\n",
    "#         print(\"No 990EZ in filing %s, skipping\" % this_object_id)\n",
    "\n",
    "    # Don't run endlessly during a demo:\n",
    "        outputdata['exec_comp'] = exec_comp\n",
    "        dw.writerow(outputdata)\n",
    "        \n",
    "#     if(num_rows > DEMO_MAX):\n",
    "#        break\n",
    "    if num_rows%100==0:\n",
    "        print(\"Processed %s filings\" % num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the outfile\n",
    "outfile.close()\n",
    "\n",
    "# sked_j_ore_efilers = pd.read_csv(outfilename)\n",
    "# sked_j_ore_efilers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
